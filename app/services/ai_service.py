"""–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Google Generative AI (Gemini)."""
import io
from pathlib import Path
from typing import List, Optional

from google import genai
from google.genai import types
from google.api_core.exceptions import ResourceExhausted
import numpy as np
from sqlalchemy.ext.asyncio import AsyncSession

try:
    import fitz  # PyMuPDF –¥–ª—è —á—Ç–µ–Ω–∏—è PDF (–∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç—Å—è –∫–∞–∫ fitz)
except ImportError:
    fitz = None  # type: ignore

try:
    from pydub import AudioSegment
    PYDUB_AVAILABLE = True
    # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: pydub —Ç—Ä–µ–±—É–µ—Ç ffmpeg –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –∞—É–¥–∏–æ —Ñ–æ—Ä–º–∞—Ç–æ–≤
    # –ï—Å–ª–∏ ffmpeg –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –Ω–µ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å, –Ω–æ —ç—Ç–æ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ
except ImportError:
    PYDUB_AVAILABLE = False
    AudioSegment = None  # type: ignore

try:
    from docx import Document
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False
    Document = None  # type: ignore

from app.core.config import settings
from app.services.vector_store import VectorStore
from app.services.chat_history import (
    get_recent_messages,
    save_message,
    format_history_for_prompt,
)
from app.utils.logger import logger

# –°–æ–æ–±—â–µ–Ω–∏–µ –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ –∫–≤–æ—Ç—ã
QUOTA_EXCEEDED_MESSAGE = "‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–æ–≤! –ú–æ–∑–≥—É –Ω—É–∂–Ω–æ –æ—Ç–¥–æ—Ö–Ω—É—Ç—å 15 —Å–µ–∫—É–Ω–¥. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–≤—Ç–æ—Ä–∏ –∑–∞–ø—Ä–æ—Å —á—É—Ç—å –ø–æ–∑–∂–µ."

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ Google GenAI (–µ—Å–ª–∏ –∫–ª—é—á –∑–∞–¥–∞–Ω)
gemini_client: Optional[genai.Client] = None
if settings.gemini_api_key:
    try:
        gemini_client = genai.Client(api_key=settings.gemini_api_key)
        
        # –í—ã–≤–æ–¥ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        available_models = [m.name for m in gemini_client.models.list()]
        print(f"[DEBUG] Available Gemini models: {available_models}")
        logger.info(f"[DEBUG] Available Gemini models: {available_models}")
    except Exception as e:
        logger.warning(f"[DEBUG] Failed to initialize Gemini client: {e}")
        gemini_client = None


class GeminiService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Google Generative AI (Gemini)."""
    
    # –í–µ–∫—Ç–æ—Ä–Ω—ã–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –ø–æ –æ—Ç–¥–µ–ª–∞–º (multitenancy)
    # –§–æ—Ä–º–∞—Ç: {'delivery/courier': VectorStore, 'sorting': VectorStore, ...}
    _vector_stores: dict[str, VectorStore] = {}
    
    # –°—Ç–∞—Ä–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ (deprecated)
    _vector_store: VectorStore | None = None
    
    @staticmethod
    def rebuild_index_for_department(department: str) -> None:
        """
        –¢–æ—á–µ—á–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –æ—Ç–¥–µ–ª–∞ (–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è).
        
        Args:
            department: –ù–∞–∑–≤–∞–Ω–∏–µ –æ—Ç–¥–µ–ª–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'sorting', 'manager', 'delivery/courier')
        """
        try:
            logger.info(f"[RAG] üéØ –¢–æ—á–µ—á–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –æ—Ç–¥–µ–ª–∞: {department}")
            
            from app.core.models import Department as DepartmentEnum
            import fitz
            
            knowledge_path = Path("data/knowledge")
            text_extensions = {".txt", ".md", ".rst"}
            pdf_extensions = {".pdf"}
            docx_extensions = {".docx"}
            
            # –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∂–∞–µ–º common —Ñ–∞–π–ª—ã
            common_path = knowledge_path / "common"
            common_chunks: List[str] = []
            common_metadata: List[dict] = []
            
            if common_path.exists() and common_path.is_dir():
                for file_path in common_path.iterdir():
                    if not file_path.is_file():
                        continue
                    
                    file_ext = file_path.suffix.lower()
                    content = ""
                    
                    try:
                        if file_ext in text_extensions:
                            content = file_path.read_text(encoding="utf-8")
                        elif file_ext in pdf_extensions:
                            if fitz is None:
                                continue
                            doc = fitz.open(file_path)
                            content = "\n".join([page.get_text() for page in doc])
                            doc.close()
                        elif file_ext in docx_extensions:
                            if not DOCX_AVAILABLE or Document is None:
                                continue
                            doc = Document(file_path)
                            content = "\n".join([p.text for p in doc.paragraphs if p.text.strip()])
                        else:
                            continue
                        
                        if content:
                            file_chunks = GeminiService._split_text_into_chunks(content, chunk_size=1000, overlap=200)
                            common_chunks.extend(file_chunks)
                            common_metadata.extend([{"filename": f"common/{file_path.name}"} for _ in file_chunks])
                    
                    except Exception as e:
                        logger.warning(f"[RAG] Failed to process common/{file_path.name}: {e}")
            
            # –¢–µ–ø–µ—Ä—å –∑–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –æ—Ç–¥–µ–ª–∞
            dept_chunks: List[str] = []
            dept_metadata: List[dict] = []
            
            dept_chunks.extend(common_chunks)
            dept_metadata.extend(common_metadata)
            
            dept_path = knowledge_path / department
            if dept_path.exists() and dept_path.is_dir():
                for file_path in dept_path.rglob("*"):
                    if not file_path.is_file():
                        continue
                    
                    file_ext = file_path.suffix.lower()
                    content = ""
                    
                    try:
                        if file_ext in text_extensions:
                            content = file_path.read_text(encoding="utf-8")
                        elif file_ext in pdf_extensions:
                            if fitz is None:
                                continue
                            doc = fitz.open(file_path)
                            content = "\n".join([page.get_text() for page in doc])
                            doc.close()
                        elif file_ext in docx_extensions:
                            if not DOCX_AVAILABLE or Document is None:
                                continue
                            doc = Document(file_path)
                            content = "\n".join([p.text for p in doc.paragraphs if p.text.strip()])
                        else:
                            continue
                        
                        if content:
                            file_chunks = GeminiService._split_text_into_chunks(content, chunk_size=1000, overlap=200)
                            dept_chunks.extend(file_chunks)
                            dept_metadata.extend([{"filename": f"{department}/{file_path.name}"} for _ in file_chunks])
                            logger.info(f"[RAG] Processed {department}/{file_path.name}: {len(file_chunks)} chunks")
                    
                    except Exception as e:
                        logger.warning(f"[RAG] Failed to process {department}/{file_path.name}: {e}")
            
            # –°–æ–∑–¥–∞–µ–º/–æ–±–Ω–æ–≤–ª—è–µ–º –∏–Ω–¥–µ–∫—Å –æ—Ç–¥–µ–ª–∞
            if dept_chunks:
                logger.info(f"[RAG] Department {department}: {len(dept_chunks)} chunks total")
                embeddings = GeminiService._generate_embeddings(dept_chunks)
                
                if len(embeddings) != len(dept_chunks):
                    dept_chunks = dept_chunks[:embeddings.shape[0]]
                    dept_metadata = dept_metadata[:embeddings.shape[0]]
                
                vector_store = VectorStore()
                vector_store.clear()
                vector_store.add_embeddings(embeddings, dept_chunks, dept_metadata)
                GeminiService._vector_stores[department] = vector_store
                logger.info(f"[RAG] ‚úÖ Index updated for {department}: {len(dept_chunks)} chunks")
            else:
                logger.warning(f"[RAG] No chunks for department {department}")
        
        except Exception as e:
            logger.error(f"[RAG] Error rebuilding index for {department}: {e}", exc_info=True)
    
    @staticmethod
    def _create_department_indices() -> None:
        """
        –°–æ–∑–¥–∞–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ—Ç–¥–µ–ª–∞ (multitenancy).
        –ö–∞–∂–¥—ã–π –∏–Ω–¥–µ–∫—Å –≤–∫–ª—é—á–∞–µ—Ç: —Ñ–∞–π–ª—ã –æ—Ç–¥–µ–ª–∞ + —Ñ–∞–π–ª—ã –∏–∑ common/.
        """
        try:
            logger.info("[RAG] Creating department-based vector indices...")
            
            from app.core.models import Department
            
            knowledge_path = Path("data/knowledge")
            if not knowledge_path.exists():
                logger.warning("[RAG] Knowledge base directory not found")
                GeminiService._vector_stores = {}
                return
            
            # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
            text_extensions = {".txt", ".md", ".rst"}
            pdf_extensions = {".pdf"}
            docx_extensions = {".docx"}
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –æ—Ç–¥–µ–ª–æ–≤
            departments = [dept.value for dept in Department]
            
            # –°–Ω–∞—á–∞–ª–∞ —á–∏—Ç–∞–µ–º common —Ñ–∞–π–ª—ã (–æ–Ω–∏ –±—É–¥—É—Ç –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤–æ –≤—Å–µ –∏–Ω–¥–µ–∫—Å—ã)
            common_path = knowledge_path / "common"
            common_chunks: List[str] = []
            common_metadata: List[dict] = []
            
            if common_path.exists() and common_path.is_dir():
                logger.info("[RAG] Loading common knowledge...")
                for file_path in common_path.iterdir():
                    if not file_path.is_file():
                        continue
                    
                    file_ext = file_path.suffix.lower()
                    content = ""
                    
                    try:
                        # –ß–∏—Ç–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã
                        if file_ext in text_extensions:
                            content = file_path.read_text(encoding="utf-8")
                        
                        # –ß–∏—Ç–∞–µ–º PDF —Ñ–∞–π–ª—ã
                        elif file_ext in pdf_extensions:
                            if fitz is None:
                                logger.warning(f"[RAG] PyMuPDF not installed, skipping PDF: {file_path.name}")
                                continue
                            
                            doc = fitz.open(file_path)
                            text_parts: List[str] = []
                            for page in doc:
                                text_parts.append(page.get_text())
                            content = "\n".join(text_parts)
                            doc.close()
                        
                        # –ß–∏—Ç–∞–µ–º DOCX —Ñ–∞–π–ª—ã
                        elif file_ext in docx_extensions:
                            if not DOCX_AVAILABLE or Document is None:
                                logger.warning(f"[RAG] python-docx not installed, skipping DOCX: {file_path.name}")
                                continue
                            
                            doc = Document(file_path)
                            text_parts: List[str] = []
                            for paragraph in doc.paragraphs:
                                if paragraph.text.strip():
                                    text_parts.append(paragraph.text)
                            content = "\n".join(text_parts)
                        
                        else:
                            continue
                        
                        if content:
                            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
                            file_chunks = GeminiService._split_text_into_chunks(content, chunk_size=1000, overlap=200)
                            common_chunks.extend(file_chunks)
                            common_metadata.extend([{"filename": f"common/{file_path.name}"} for _ in file_chunks])
                            logger.info(f"[RAG] Processed common/{file_path.name}: {len(file_chunks)} chunks")
                    
                    except Exception as e:
                        logger.warning(f"[RAG] Failed to process common/{file_path.name}: {e}")
            
            logger.info(f"[RAG] Common knowledge: {len(common_chunks)} chunks")
            
            # –¢–µ–ø–µ—Ä—å —Å–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ—Ç–¥–µ–ª–∞
            for department in departments:
                try:
                    logger.info(f"[RAG] Creating index for department: {department}")
                    
                    dept_chunks: List[str] = []
                    dept_metadata: List[dict] = []
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º common —á–∞–Ω–∫–∏
                    dept_chunks.extend(common_chunks)
                    dept_metadata.extend(common_metadata)
                    
                    # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª—ã –æ—Ç–¥–µ–ª–∞
                    dept_path = knowledge_path / department
                    if dept_path.exists() and dept_path.is_dir():
                        for file_path in dept_path.rglob("*"):
                            if not file_path.is_file():
                                continue
                            
                            file_ext = file_path.suffix.lower()
                            content = ""
                            
                            try:
                                if file_ext in text_extensions:
                                    content = file_path.read_text(encoding="utf-8")
                                elif file_ext in pdf_extensions:
                                    if fitz is None:
                                        continue
                                    doc = fitz.open(file_path)
                                    content = "\n".join([page.get_text() for page in doc])
                                    doc.close()
                                elif file_ext in docx_extensions:
                                    if not DOCX_AVAILABLE or Document is None:
                                        continue
                                    doc = Document(file_path)
                                    content = "\n".join([p.text for p in doc.paragraphs if p.text.strip()])
                                else:
                                    continue
                                
                                if content:
                                    file_chunks = GeminiService._split_text_into_chunks(content, chunk_size=1000, overlap=200)
                                    dept_chunks.extend(file_chunks)
                                    dept_metadata.extend([{"filename": f"{department}/{file_path.name}"} for _ in file_chunks])
                                    logger.info(f"[RAG] Processed {department}/{file_path.name}: {len(file_chunks)} chunks")
                            
                            except Exception as e:
                                logger.warning(f"[RAG] Failed to process {department}/{file_path.name}: {e}")
                    
                    # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è –æ—Ç–¥–µ–ª–∞
                    if dept_chunks:
                        logger.info(f"[RAG] Department {department}: {len(dept_chunks)} chunks total")
                        embeddings = GeminiService._generate_embeddings(dept_chunks)
                        
                        if len(embeddings) != len(dept_chunks):
                            dept_chunks = dept_chunks[:embeddings.shape[0]]
                            dept_metadata = dept_metadata[:embeddings.shape[0]]
                        
                        vector_store = VectorStore()
                        vector_store.clear()
                        vector_store.add_embeddings(embeddings, dept_chunks, dept_metadata)
                        GeminiService._vector_stores[department] = vector_store
                        logger.info(f"[RAG] Index created for {department}: {len(dept_chunks)} chunks")
                    else:
                        logger.warning(f"[RAG] No chunks for department {department}")
                
                except Exception as e:
                    logger.error(f"[RAG] Error creating index for {department}: {e}", exc_info=True)
            
            logger.info(f"[RAG] Created {len(GeminiService._vector_stores)} department indices")
            
            # Fallback: —Å–æ–∑–¥–∞–µ–º —Å—Ç–∞—Ä—ã–π –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            if common_chunks:
                all_chunks = common_chunks.copy()
                all_metadata = common_metadata.copy()
                embeddings = GeminiService._generate_embeddings(all_chunks)
                if len(embeddings) != len(all_chunks):
                    all_chunks = all_chunks[:embeddings.shape[0]]
                    all_metadata = all_metadata[:embeddings.shape[0]]
                vector_store = VectorStore()
                vector_store.clear()
                vector_store.add_embeddings(embeddings, all_chunks, all_metadata)
                GeminiService._vector_store = vector_store
                logger.info(f"[RAG] Fallback global index created with {len(all_chunks)} chunks")
            
        except Exception as e:
            logger.error(f"[RAG] Error creating vector index: {e}", exc_info=True)
            GeminiService._vector_store = None
    
    @staticmethod
    def _load_knowledge_base() -> str:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–æ–≤ –∏–∑ –ø–∞–ø–∫–∏ data/knowledge.
        
        Returns:
            –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ –ø–∞–ø–∫–∏ knowledge
        """
        knowledge_path = Path("data/knowledge")
        if not knowledge_path.exists():
            logger.info("Knowledge base directory not found")
            return ""
        
        context_parts: List[str] = []
        
        # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
        text_extensions = {".txt", ".md", ".rst"}
        pdf_extensions = {".pdf"}
        
        # –ß–∏—Ç–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –∏–∑ –ø–∞–ø–∫–∏ knowledge
        for file_path in knowledge_path.iterdir():
            if not file_path.is_file():
                continue
                
            file_ext = file_path.suffix.lower()
            content = ""
            
            try:
                # –ß–∏—Ç–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã
                if file_ext in text_extensions:
                    content = file_path.read_text(encoding="utf-8")
                    
                # –ß–∏—Ç–∞–µ–º PDF —Ñ–∞–π–ª—ã
                elif file_ext in pdf_extensions:
                    if fitz is None:
                        logger.warning(f"PyMuPDF not installed, skipping PDF file: {file_path.name}")
                        continue
                    
                    doc = fitz.open(file_path)
                    text_parts: List[str] = []
                    for page in doc:
                        text_parts.append(page.get_text())
                    content = "\n".join(text_parts)
                    doc.close()
                    
                else:
                    logger.debug(f"Unsupported file type: {file_path.name}")
                    continue
                
                if content:
                    context_parts.append(f"–§–∞–π–ª: {file_path.name}\n{content}\n")
                    logger.info(f"Loaded knowledge file: {file_path.name}")
                    
            except Exception as e:
                logger.warning(f"Failed to read file {file_path.name}: {e}")
        
        if not context_parts:
            logger.info("No text files found in knowledge base")
            return ""
        
        return "\n---\n".join(context_parts)
    
    @staticmethod
    def _split_text_into_chunks(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
        """
        –†–∞–∑—Ä–µ–∑–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –∫—É—Å–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º.
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö (1000-1500)
            overlap: –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏ –≤ —Å–∏–º–≤–æ–ª–∞—Ö (200)
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤
        """
        if not text:
            return []
        
        chunks: List[str] = []
        start = 0
        text_length = len(text)
        
        while start < text_length:
            end = start + chunk_size
            chunk = text[start:end]
            
            # –ï—Å–ª–∏ –Ω–µ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫, –ø—ã—Ç–∞–µ–º—Å—è –∑–∞–∫–æ–Ω—á–∏—Ç—å –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            if end < text_length:
                # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Ç–æ—á–∫—É, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∏–ª–∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –∑–Ω–∞–∫
                last_sentence_end = max(
                    chunk.rfind('.'),
                    chunk.rfind('!'),
                    chunk.rfind('?'),
                    chunk.rfind('\n')
                )
                if last_sentence_end > chunk_size * 0.5:  # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø–æ–ª–æ–≤–∏–Ω–µ
                    chunk = chunk[:last_sentence_end + 1]
                    end = start + last_sentence_end + 1
            
            chunks.append(chunk.strip())
            
            # –°–ª–µ–¥—É—é—â–∏–π —á–∞–Ω–∫ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
            start = end - overlap
            if start >= text_length:
                break
        
        logger.info(f"Split text into {len(chunks)} chunks (size: {chunk_size}, overlap: {overlap})")
        return chunks
    
    @staticmethod
    def _generate_embeddings(texts: List[str]) -> np.ndarray:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ —á–µ—Ä–µ–∑ –Ω–æ–≤—ã–π API google.genai.
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        
        Returns:
            –ú–∞—Å—Å–∏–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (numpy array, shape: [n_texts, dimension])
        """
        try:
            if gemini_client is None:
                raise ValueError("Gemini client not initialized")
            
            embeddings_list: List[List[float]] = []
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ –Ω–æ–≤—ã–π API
            for i, text in enumerate(texts):
                try:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º gemini-embedding-001 –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
                    result = gemini_client.models.embed_content(
                        model="gemini-embedding-001",
                        contents=text,
                        config=types.EmbedContentConfig(
                            task_type="RETRIEVAL_DOCUMENT"
                        )
                    )
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                    if hasattr(result, 'embeddings') and len(result.embeddings) > 0:
                        embedding = result.embeddings[0].values
                        embeddings_list.append(embedding)
                    else:
                        logger.warning(f"No embeddings in result for text {i}")
                        continue
                    
                    if (i + 1) % 10 == 0:
                        logger.info(f"Generated embeddings for {i + 1}/{len(texts)} texts")
                        
                except Exception as e:
                    logger.error(f"Error generating embedding for text {i}: {e}")
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç –≤–º–µ—Å—Ç–æ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω—É–ª–µ–≤–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞
                    logger.warning(f"Skipping text {i} due to embedding error")
                    continue
            
            if not embeddings_list:
                raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞")
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —á–∞–Ω–∫–∏, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            if len(embeddings_list) != len(texts):
                logger.warning(f"Generated {len(embeddings_list)} embeddings for {len(texts)} texts (some were skipped)")
            
            embeddings_array = np.array(embeddings_list, dtype=np.float32)
            logger.info(f"Generated {len(embeddings_list)} embeddings (shape: {embeddings_array.shape})")
            return embeddings_array
            
        except Exception as e:
            logger.error(f"Error in _generate_embeddings: {e}", exc_info=True)
            raise Exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {str(e)}")
    
    @staticmethod
    def create_vector_db() -> None:
        """
        –°–æ–∑–¥–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–æ–≤ knowledge.
        –ß–∏—Ç–∞–µ—Ç –≤—Å–µ —Ñ–∞–π–ª—ã, —Ä–∞–∑–±–∏–≤–∞–µ—Ç –Ω–∞ —á–∞–Ω–∫–∏, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ FAISS.
        """
        try:
            logger.info("[VECTOR_DB] Starting vector database creation...")
            
            knowledge_path = Path("data/knowledge")
            if not knowledge_path.exists():
                logger.warning("[VECTOR_DB] Knowledge base directory not found")
                return
            
            # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
            text_extensions = {".txt", ".md", ".rst"}
            pdf_extensions = {".pdf"}
            docx_extensions = {".docx"}
            
            all_chunks: List[str] = []
            
            # –ß–∏—Ç–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –∏–∑ –ø–∞–ø–∫–∏ knowledge
            for file_path in knowledge_path.iterdir():
                if not file_path.is_file():
                    continue
                
                file_ext = file_path.suffix.lower()
                content = ""
                
                try:
                    # –ß–∏—Ç–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã
                    if file_ext in text_extensions:
                        content = file_path.read_text(encoding="utf-8")
                    
                    # –ß–∏—Ç–∞–µ–º PDF —Ñ–∞–π–ª—ã
                    elif file_ext in pdf_extensions:
                        if fitz is None:
                            logger.warning(f"[VECTOR_DB] PyMuPDF not installed, skipping PDF: {file_path.name}")
                            continue
                        
                        doc = fitz.open(file_path)
                        text_parts: List[str] = []
                        for page in doc:
                            text_parts.append(page.get_text())
                        content = "\n".join(text_parts)
                        doc.close()
                    
                    # –ß–∏—Ç–∞–µ–º DOCX —Ñ–∞–π–ª—ã
                    elif file_ext in docx_extensions:
                        if not DOCX_AVAILABLE or Document is None:
                            logger.warning(f"[VECTOR_DB] python-docx not installed, skipping DOCX: {file_path.name}")
                            continue
                        
                        doc = Document(file_path)
                        text_parts: List[str] = []
                        for paragraph in doc.paragraphs:
                            if paragraph.text.strip():
                                text_parts.append(paragraph.text)
                        content = "\n".join(text_parts)
                    
                    else:
                        logger.debug(f"[VECTOR_DB] Unsupported file type: {file_path.name}")
                        continue
                    
                    if content:
                        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
                        file_chunks = GeminiService._split_text_into_chunks(
                            content,
                            chunk_size=1200,
                            overlap=200
                        )
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∞–π–ª–µ –∫ –∫–∞–∂–¥–æ–º—É —á–∞–Ω–∫—É
                        file_chunks_with_meta = [
                            f"[–§–∞–π–ª: {file_path.name}]\n{chunk}"
                            for chunk in file_chunks
                        ]
                        
                        all_chunks.extend(file_chunks_with_meta)
                        logger.info(f"[VECTOR_DB] Processed {file_path.name}: {len(file_chunks)} chunks")
                    
                except Exception as e:
                    logger.warning(f"[VECTOR_DB] Failed to process {file_path.name}: {e}")
            
            if not all_chunks:
                logger.warning("[VECTOR_DB] No chunks to process")
                return
            
            logger.info(f"[VECTOR_DB] Total chunks: {len(all_chunks)}")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            logger.info("[VECTOR_DB] Generating embeddings...")
            embeddings = GeminiService._generate_embeddings(all_chunks)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —á–∞–Ω–∫–∏, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            # (–µ—Å–ª–∏ –±—ã–ª–∏ –ø—Ä–æ–ø—É—â–µ–Ω—ã –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç–µ–∫—Å—Ç—ã)
            if embeddings.shape[0] < len(all_chunks):
                logger.warning(f"[VECTOR_DB] Some chunks were skipped: {embeddings.shape[0]} embeddings for {len(all_chunks)} chunks")
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ —á–∞–Ω–∫–∏, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –µ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
                all_chunks = all_chunks[:embeddings.shape[0]]
            
            # –°–æ–∑–¥–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
            vector_store = VectorStore()
            vector_store.clear()  # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
            vector_store.add_embeddings(embeddings, all_chunks)
            vector_store.save_index()
            
            logger.info(f"[VECTOR_DB] Vector database created successfully with {len(all_chunks)} chunks")
            
        except Exception as e:
            logger.error(f"[VECTOR_DB] Error creating vector database: {e}", exc_info=True)
            raise Exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã: {str(e)}")
    
    @staticmethod
    def _is_russian_text(text: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ —Ä—É—Å—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã (–∫–∏—Ä–∏–ª–ª–∏—Ü—É).
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            
        Returns:
            True –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º, False –∏–Ω–∞—á–µ
        """
        if not text:
            return True
        
        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã
        return any('–∞' <= char <= '—è' or '–ê' <= char <= '–Ø' for char in text)
    
    @staticmethod
    async def _translate_to_russian(text: str) -> str:
        """
        –ü–µ—Ä–µ–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ —á–µ—Ä–µ–∑ Gemini –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞ –≤ RAG.
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞
            
        Returns:
            –ü–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
        """
        try:
            if gemini_client is None:
                logger.warning("[TRANSLATE] Gemini client not initialized, returning original text")
                return text
            
            logger.info(f"[TRANSLATE] Translating query to Russian: {text[:100]}...")
            
            response = gemini_client.models.generate_content(
                model="gemini-2.5-flash",
                contents=f"–ü–µ—Ä–µ–≤–µ–¥–∏ —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ –æ–¥–Ω–æ–π —Ñ—Ä–∞–∑–æ–π, —Å–æ—Ö—Ä–∞–Ω—è—è —Å–º—ã—Å–ª:\n\n{text}",
                config=types.GenerateContentConfig(
                    temperature=0.3,
                )
            )
            
            translated = response.text.strip() if response.text else text
            logger.info(f"[TRANSLATE] Translated query: {translated}")
            return translated
            
        except Exception as e:
            logger.error(f"[TRANSLATE] Error translating text: {e}", exc_info=True)
            return text  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –ø—Ä–∏ –æ—à–∏–±–∫–µ
    
    @staticmethod
    async def get_answer(
        prompt: str,
        user_id: int,
        session: AsyncSession,
        context: str | None = None,
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–º–ø—Ç–∞ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ RAG-—Å–∏—Å—Ç–µ–º—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
        
        Args:
            prompt: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user_id: Telegram ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            session: –°–µ—Å—Å–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∏—Å—Ç–æ—Ä–∏–µ–π
            context: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–º–µ—Å—Ç–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞)
        
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏ Gemini
        
        Raises:
            Exception: –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ API –∫–ª—é—á–∞
            if not settings.gemini_api_key:
                raise ValueError("GEMINI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –µ–≥–æ –≤ .env —Ñ–∞–π–ª–µ.")
            
            # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Å–æ–æ–±—â–µ–Ω–∏–π)
            history_messages = await get_recent_messages(session, user_id, limit=10)
            history_text = format_history_for_prompt(history_messages)
            
            # –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–µ—Ä–µ–¥–∞–Ω —è–≤–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
            source_files: List[str] = []
            if context is not None:
                relevant_chunks_text = context
                source_files = []  # –ü—Ä–∏ —è–≤–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —Å —É—á–µ—Ç–æ–º –æ—Ç–¥–µ–ª–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                try:
                    # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–¥–µ–ª –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –∏–∑–æ–ª—è—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π
                    from app.utils.department import get_user_department
                    user_department = await get_user_department(session, user_id)
                    
                    logger.info(f"[RAG] User {user_id} department: {user_department or 'admin (all departments)'}")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–∑–¥–∞–Ω –ª–∏ –∏–Ω–¥–µ–∫—Å
                    if not GeminiService._vector_stores:
                        logger.info("[RAG] Vector indices not found, creating new ones...")
                        GeminiService._create_department_indices()
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —è–∑—ã–∫ –∑–∞–ø—Ä–æ—Å–∞ –∏ –ø–µ—Ä–µ–≤–æ–¥–∏–º –Ω–∞ —Ä—É—Å—Å–∫–∏–π –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
                    search_query = prompt
                    if not GeminiService._is_russian_text(prompt):
                        logger.info(f"[RAG] Query is not in Russian, translating for better search accuracy...")
                        search_query = await GeminiService._translate_to_russian(prompt)
                    
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ –Ω–æ–≤—ã–π API
                    logger.info(f"[RAG] Generating query embedding for: {search_query[:100]}...")
                    if gemini_client is None:
                        raise ValueError("Gemini client not initialized")
                    
                    query_embedding_result = gemini_client.models.embed_content(
                        model="gemini-embedding-001",
                        contents=search_query,
                        config=types.EmbedContentConfig(
                            task_type="RETRIEVAL_QUERY"
                        )
                    )
                    query_embedding = np.array(query_embedding_result.embeddings[0].values, dtype=np.float32)
                    
                    # –†–ï–ñ–ò–ú –ë–û–ì–ê –î–õ–Ø –ê–î–ú–ò–ù–ê: –ò—â–µ–º –ø–æ –í–°–ï–ú –∏–Ω–¥–µ–∫—Å–∞–º
                    if user_department is None:
                        logger.info(f"[RAG] üî• ADMIN GOD MODE: Searching across ALL department indices...")
                        all_search_results = []
                        
                        # –ò—â–µ–º –ø–æ –≤—Å–µ–º –∏–Ω–¥–µ–∫—Å–∞–º –æ—Ç–¥–µ–ª–æ–≤
                        for dept_name, dept_store in GeminiService._vector_stores.items():
                            if dept_store and dept_store.index is not None:
                                try:
                                    dept_results = dept_store.search(query_embedding, top_k=2)  # –ü–æ 2 –∏–∑ –∫–∞–∂–¥–æ–≥–æ
                                    for chunk, distance, metadata in dept_results:
                                        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—Ç–¥–µ–ª–µ –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                                        enhanced_metadata = metadata.copy() if metadata else {}
                                        enhanced_metadata['department'] = dept_name
                                        all_search_results.append((chunk, distance, enhanced_metadata))
                                except Exception as e:
                                    logger.warning(f"[RAG] Error searching in {dept_name}: {e}")
                        
                        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ distance –∏ –±–µ—Ä–µ–º top-5 –ª—É—á—à–∏—Ö
                        all_search_results.sort(key=lambda x: x[1])  # –ú–µ–Ω—å—à–µ distance = –ª—É—á—à–µ
                        search_results = all_search_results[:5]
                        logger.info(f"[RAG] Admin found {len(search_results)} chunks across {len(GeminiService._vector_stores)} departments")
                    
                    # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –æ—Ç–¥–µ–ª–æ–≤
                    elif user_department and user_department in GeminiService._vector_stores:
                        # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –æ—Ç–¥–µ–ª–æ–º –∏—â–µ—Ç –≤ —Å–≤–æ–µ–º –æ—Ç–¥–µ–ª–µ + common
                        search_results = []
                        
                        # 1. –ü–æ–∏—Å–∫ –≤ —Å–≤–æ–µ–º –æ—Ç–¥–µ–ª–µ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
                        vector_store = GeminiService._vector_stores[user_department]
                        logger.info(f"[RAG] User {user_id} (Dept: {user_department}) searching in department index...")
                        dept_results = vector_store.search(query_embedding, top_k=2)
                        search_results.extend(dept_results)
                        
                        # 2. –ü–æ–∏—Å–∫ –≤ common (–µ—Å–ª–∏ –æ–Ω —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ —ç—Ç–æ –Ω–µ —Å–∞–º common)
                        if user_department != "common" and "common" in GeminiService._vector_stores:
                            common_store = GeminiService._vector_stores["common"]
                            logger.info(f"[RAG] Also searching in 'common' for user {user_id}...")
                            common_results = common_store.search(query_embedding, top_k=2)
                            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —á—Ç–æ–±—ã –∑–Ω–∞—Ç—å —á—Ç–æ –∏–∑ common
                            for chunk, distance, metadata in common_results:
                                enhanced_metadata = metadata.copy() if metadata else {}
                                enhanced_metadata['department'] = 'common'
                                search_results.append((chunk, distance, enhanced_metadata))
                        
                        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ relevance (distance)
                        search_results.sort(key=lambda x: x[1])
                        search_results = search_results[:3]  # –¢–æ–ø-3 –∏–∑ –æ–±–æ–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                        
                        logger.info(f"[RAG] Found {len(search_results)} chunks (from {user_department} + common)")
                    
                    else:
                        logger.warning(f"[RAG] Department {user_department} not found in indices, using fallback")
                        vector_store = GeminiService._vector_store
                        if vector_store and vector_store.index is not None:
                            search_results = vector_store.search(query_embedding, top_k=3)
                        else:
                            search_results = []
                    
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
                    if not search_results:
                        logger.warning("[RAG] No relevant chunks found, using empty context")
                        relevant_chunks_text = ""
                        source_files = []
                    else:
                        # –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–Ø –¥–ª—è –∞–¥–º–∏–Ω–∞ (—á—Ç–æ–±—ã common/ –Ω–µ –¥—É–±–ª–∏—Ä–æ–≤–∞–ª—Å—è)
                        chunks_texts: List[str] = []
                        source_files_set: set[str] = set()
                        seen_chunks: set[str] = set()  # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
                        departments_used: set[str] = set()  # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–¥–µ–ª–æ–≤
                        
                        for chunk, distance, metadata in search_results:
                            # –°–æ–∑–¥–∞–µ–º —Ö–µ—à —á–∞–Ω–∫–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
                            chunk_hash = chunk.strip()[:200]  # –ü–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
                            
                            # –ï—Å–ª–∏ –∞–¥–º–∏–Ω - –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
                            if user_department is None:
                                if chunk_hash in seen_chunks:
                                    logger.debug(f"[RAG] Skipping duplicate chunk from {metadata.get('filename', 'unknown')}")
                                    continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç
                                seen_chunks.add(chunk_hash)
                                
                                # –ú–ï–¢–ö–ò –ò–°–¢–û–ß–ù–ò–ö–û–í –î–õ–Ø –ê–î–ú–ò–ù–ê
                                if metadata and "filename" in metadata:
                                    filename = metadata.get("filename", "")
                                    department_name = metadata.get("department", "unknown")
                                    
                                    # –ü–†–ò–û–†–ò–¢–ï–¢ –ò–°–¢–û–ß–ù–ò–ö–û–í:
                                    # –ï—Å–ª–∏ —Ñ–∞–π–ª –∏–∑ common/ -> "–û–±—â–∏–µ –∑–Ω–∞–Ω–∏—è"
                                    if "common/" in filename or filename.startswith("common/"):
                                        source_label = "–û–±—â–∏–µ –∑–Ω–∞–Ω–∏—è"
                                    else:
                                        # –ò–Ω–∞—á–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—Ç–¥–µ–ª
                                        source_label = department_name
                                    
                                    departments_used.add(source_label)
                                    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∫—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–µ—Ä–µ–¥ —á–∞–Ω–∫–æ–º
                                    tagged_chunk = f"[–ò—Å—Ç–æ—á–Ω–∏–∫: {source_label}]\n{chunk}"
                                    chunks_texts.append(tagged_chunk)
                                else:
                                    chunks_texts.append(chunk)
                            else:
                                # –û–±—ã—á–Ω—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å - –±–µ–∑ –º–µ—Ç–æ–∫
                                chunks_texts.append(chunk)
                            
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                            if metadata and "filename" in metadata:
                                filename = metadata["filename"]
                                # –î–ª—è –∞–¥–º–∏–Ω–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—Ç–¥–µ–ª
                                if user_department is None and "department" in metadata:
                                    filename = f"[{metadata['department']}] {filename}"
                                source_files_set.add(filename)
                        
                        relevant_chunks_text = "\n\n---\n\n".join(chunks_texts)
                        source_files = sorted(list(source_files_set))
                        
                        if user_department is None:
                            logger.info(f"[RAG] üî• Admin: Found {len(chunks_texts)} unique chunks (after deduplication) from {len(source_files)} files")
                            logger.info(f"[RAG] üè¢ Departments used: {sorted(departments_used)}")
                        else:
                            logger.info(f"[RAG] Found {len(search_results)} relevant chunks from {len(source_files)} files: {source_files}")
                
                except Exception as e:
                    logger.error(f"[RAG] Error in vector search: {e}", exc_info=True)
                    # Fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥
                    logger.info("[RAG] Falling back to full text search...")
                    relevant_chunks_text = GeminiService._load_knowledge_base()
                    source_files = []  # –í fallback —Ä–µ–∂–∏–º–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º
            
            # –°–∏—Å—Ç–µ–º–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ—Å—Ç–∏
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∞–¥–º–∏–Ω –ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å
            is_admin = user_department is None if 'user_department' in locals() else False
            
            system_instruction = """–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ UQsoft. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ø–æ–º–æ–≥–∞—Ç—å —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞–º –Ω–∞—Ö–æ–¥–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –∫–æ–º–ø–∞–Ω–∏–∏.

–ö–†–ò–¢–ò–ß–ù–û - –Ø–ó–´–ö –û–¢–í–ï–¢–ê:
–û–ø—Ä–µ–¥–µ–ª—è–π —è–∑—ã–∫ –≤–æ–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –æ—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –Ω–∞ —Ç–æ–º –∂–µ —è–∑—ã–∫–µ (—Ä—É—Å—Å–∫–∏–π, –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –∏–ª–∏ –∫–∏—Ç–∞–π—Å–∫–∏–π). 
–ò—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –Ω–æ –ø–µ—Ä–µ–≤–æ–¥–∏ –µ—ë –Ω–∞ —è–∑—ã–∫ –∑–∞–ø—Ä–æ—Å–∞, –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ.

–ü—Ä–∏–º–µ—Ä—ã:
- –í–æ–ø—Ä–æ—Å –Ω–∞ —Ä—É—Å—Å–∫–æ–º ‚Üí –û—Ç–≤–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º (–ø–µ—Ä–µ–≤–æ–¥–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –µ—Å–ª–∏ –æ–Ω –Ω–∞ –¥—Ä—É–≥–æ–º —è–∑—ã–∫–µ)
- Question in English ‚Üí Answer in English (translate context if needed)
- ‰∏≠ÊñáÈóÆÈ¢ò ‚Üí ‰∏≠ÊñáÂõûÁ≠î (ÁøªËØë‰∏ä‰∏ãÊñáÂ¶ÇÊûúÈúÄË¶Å)

–í–ê–ñ–ù–û: –ï—Å–ª–∏ –Ω–µ —É–≤–µ—Ä–µ–Ω –≤ —è–∑—ã–∫–µ –≤–æ–ø—Ä–æ—Å–∞ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é."""
            
            # –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ô –§–û–†–ú–ê–¢ –î–õ–Ø –ê–î–ú–ò–ù–ê
            if is_admin:
                system_instruction += """

üìÇ –§–û–†–ú–ê–¢ –î–õ–Ø –ê–î–ú–ò–ù–ò–°–¢–†–ê–¢–û–†–ê (–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û):
–¢—ã –≤–∏–¥–∏—à—å —Ç–µ–≥–∏ [–ò—Å—Ç–æ—á–Ω–∏–∫: ...] –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç —á—Ç–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤–∑—è—Ç–∞ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –æ—Ç–¥–µ–ª–æ–≤ –∫–æ–º–ø–∞–Ω–∏–∏.
–í –ö–û–ù–¶–ï —Å–≤–æ–µ–≥–æ –æ—Ç–≤–µ—Ç–∞ (—Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏) –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –Ω–∞–ø–∏—à–∏:

üìÇ **–ò—Å—Ç–æ—á–Ω–∏–∫–∏:** [—Å–ø–∏—Å–æ–∫ –æ—Ç–¥–µ–ª–æ–≤ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é]

–ü—Ä–∏–º–µ—Ä:
[–ò—Å—Ç–æ—á–Ω–∏–∫: –û–±—â–∏–µ –∑–Ω–∞–Ω–∏—è] –ü–∞—Ä–æ–ª—å Wi-Fi...
[–ò—Å—Ç–æ—á–Ω–∏–∫: sorting] –ö–æ–¥ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏...

–¢–≤–æ–π –æ—Ç–≤–µ—Ç –î–û–õ–ñ–ï–ù –∑–∞–∫–∞–Ω—á–∏–≤–∞—Ç—å—Å—è –Ω–∞:

üìÇ **–ò—Å—Ç–æ—á–Ω–∏–∫–∏:** –û–±—â–∏–µ –∑–Ω–∞–Ω–∏—è, sorting"""
            else:
                system_instruction += """

–ú–ï–¢–ö–ò –ò–°–¢–û–ß–ù–ò–ö–û–í:
–ï—Å–ª–∏ —Ç—ã –≤–∏–¥–∏—à—å —Ç–µ–≥–∏ [–ò—Å—Ç–æ—á–Ω–∏–∫: ...], —ç—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.
–ú–æ–∂–µ—à—å —É–ø–æ–º—è–Ω—É—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ –∫–æ–Ω—Ü–µ –æ—Ç–≤–µ—Ç–∞: "üìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏: –∏—Å—Ç–æ—á–Ω–∏–∫1, –∏—Å—Ç–æ—á–Ω–∏–∫2" """
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, –∏—Å—Ç–æ—Ä–∏–µ–π –∏ –≤–æ–ø—Ä–æ—Å–æ–º
            prompt_parts: List[str] = []
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
            if history_text:
                prompt_parts.append(f"–ò—Å—Ç–æ—Ä–∏—è –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –¥–∏–∞–ª–æ–≥–∞:\n{history_text}\n")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
            if relevant_chunks_text and source_files:
                # –ï—Å–ª–∏ –µ—Å—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏, –¥–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –æ–± –∏—Ö –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–∏
                sources_text = ", ".join(source_files)
                prompt_parts.append(
                    "–ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∑–Ω–∞–Ω–∏–π, —á—Ç–æ–±—ã –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å. "
                    "–ï—Å–ª–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ, —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏.\n\n"
                    f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{relevant_chunks_text}\n\n"
                )
                prompt_parts.append(f"–í–æ–ø—Ä–æ—Å: {prompt}\n\n")
                prompt_parts.append(
                    f"–í–ê–ñ–ù–û: –í –∫–æ–Ω—Ü–µ –æ—Ç–≤–µ—Ç–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–±–∞–≤—å —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ:\n"
                    f"–ò—Å—Ç–æ—á–Ω–∏–∫–∏: {sources_text}"
                )
            elif relevant_chunks_text:
                # –ï—Å–ª–∏ –µ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç, –Ω–æ –Ω–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (—Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç)
                prompt_parts.append(
                    "–ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∑–Ω–∞–Ω–∏–π, —á—Ç–æ–±—ã –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å. "
                    "–ï—Å–ª–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ, —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏.\n\n"
                    f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{relevant_chunks_text}\n\n"
                    f"–í–æ–ø—Ä–æ—Å: {prompt}"
                )
            else:
                # –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –Ω–µ—Ç, –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–µ –Ω—É–∂–Ω—ã
                prompt_parts.append(
                    "–í –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –ø–æ–∫–∞ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.\n\n"
                    f"–í–æ–ø—Ä–æ—Å: {prompt}"
                )
            
            full_prompt = "\n".join(prompt_parts)
            
            logger.info(f"[GEMINI] Generating response with Gemini for prompt: {prompt[:100]}...")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª–∏–µ–Ω—Ç–∞
            if gemini_client is None:
                raise ValueError("Gemini client not initialized")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ –Ω–æ–≤—ã–π API
            logger.info("[GEMINI] Generating content with gemini-2.5-flash...")
            response = gemini_client.models.generate_content(
                model="gemini-2.5-flash",
                contents=full_prompt,
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction,
                    temperature=0.7,
                )
            )
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
            response_text = response.text if response.text else "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç."
            
            logger.info(f"[GEMINI] Successfully generated response (length: {len(response_text)})")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –æ—Ç–≤–µ—Ç –±–æ—Ç–∞ –≤ –∏—Å—Ç–æ—Ä–∏—é
            try:
                await save_message(session, user_id, "user", prompt)
                await save_message(session, user_id, "assistant", response_text)
                logger.info(f"[CHAT_HISTORY] Saved question and answer for user_id={user_id}")
            except Exception as e:
                logger.error(f"[CHAT_HISTORY] Failed to save history: {e}", exc_info=True)
                # –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é
            
            return response_text
        
        except ResourceExhausted as e:
            logger.warning(f"[GEMINI] Quota exceeded (429): {e}")
            return QUOTA_EXCEEDED_MESSAGE
            
        except Exception as e:
            logger.error(f"Error generating response with Gemini: {e}", exc_info=True)
            raise Exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}")
    
    @staticmethod
    async def get_answer_from_audio_with_rag(
        audio_bytes: bytes | None = None,
        audio_file_path: str | None = None,
        audio_mime_type: str = "audio/ogg",
        user_id: int | None = None,
        session: AsyncSession | None = None
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞ –° RAG –ü–û–ò–°–ö–û–ú.
        –î–≤—É—Ö—ç—Ç–∞–ø–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞:
        1. –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∞—É–¥–∏–æ -> —Ç–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞
        2. RAG –ø–æ–∏—Å–∫ –ø–æ —Ç–µ–∫—Å—Ç—É -> —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        
        Args:
            audio_bytes: –ë–∞–π—Ç—ã –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞
            audio_file_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É
            audio_mime_type: MIME —Ç–∏–ø (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é audio/ogg)
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è RAG –ø–æ–∏—Å–∫–∞ –ø–æ –æ—Ç–¥–µ–ª—É
            session: AsyncSession –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ë–î
        
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏ Gemini
        """
        try:
            logger.info(f"[VOICE_RAG] Starting audio processing with RAG: file_path={audio_file_path}")
            
            if gemini_client is None:
                raise ValueError("Gemini client not initialized")
            
            # –≠–¢–ê–ü 1: –¢–†–ê–ù–°–ö–†–ò–ü–¶–ò–Ø –ê–£–î–ò–û
            logger.info("[VOICE_RAG] Step 1: Transcribing audio to text...")
            transcribed_text = await GeminiService._transcribe_audio(
                audio_bytes=audio_bytes,
                audio_file_path=audio_file_path,
                audio_mime_type=audio_mime_type
            )
            logger.info(f"[VOICE_RAG] Transcribed text: {transcribed_text[:200]}...")
            
            # –≠–¢–ê–ü 2: RAG –ü–û–ò–°–ö –ü–û –¢–ï–ö–°–¢–£
            logger.info("[VOICE_RAG] Step 2: Performing RAG search...")
            if user_id and session:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ RAG –º–µ—Ö–∞–Ω–∏–∑–º —á—Ç–æ –∏ –≤ get_answer
                from app.utils.department import get_user_department
                user_department = await get_user_department(session, user_id)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —è–∑—ã–∫ –∏ –ø–µ—Ä–µ–≤–æ–¥–∏–º –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
                search_query = transcribed_text
                if not GeminiService._is_russian_text(transcribed_text):
                    logger.info("[VOICE_RAG] Translating query to Russian for better RAG accuracy...")
                    search_query = await GeminiService._translate_to_russian(transcribed_text)
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
                query_embedding_result = gemini_client.models.embed_content(
                    model="gemini-embedding-001",
                    contents=search_query,
                    config=types.EmbedContentConfig(task_type="RETRIEVAL_QUERY")
                )
                query_embedding = np.array(query_embedding_result.embeddings[0].values, dtype=np.float32)
                
                # –ü–æ–∏—Å–∫ –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º (God Mode –¥–ª—è –∞–¥–º–∏–Ω–∞ –∏–ª–∏ –ø–æ –æ—Ç–¥–µ–ª—É)
                search_results = []
                departments_used: set[str] = set()
                
                if user_department is None:  # –ê–¥–º–∏–Ω - –ø–æ–∏—Å–∫ –≤–µ–∑–¥–µ
                    logger.info("[VOICE_RAG] üî• Admin God Mode for voice!")
                    for dept_name, dept_store in GeminiService._vector_stores.items():
                        if dept_store and dept_store.index:
                            try:
                                dept_results = dept_store.search(query_embedding, top_k=2)
                                for chunk, distance, metadata in dept_results:
                                    enhanced_metadata = metadata.copy() if metadata else {}
                                    enhanced_metadata['department'] = dept_name
                                    search_results.append((chunk, distance, enhanced_metadata))
                            except Exception as e:
                                logger.warning(f"[VOICE_RAG] Error searching {dept_name}: {e}")
                    search_results.sort(key=lambda x: x[1])
                    search_results = search_results[:5]
                elif user_department in GeminiService._vector_stores:
                    vector_store = GeminiService._vector_stores[user_department]
                    search_results = vector_store.search(query_embedding, top_k=3)
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å –º–µ—Ç–∫–∞–º–∏
                chunks_texts = []
                seen_chunks = set()
                for chunk, distance, metadata in search_results:
                    chunk_hash = chunk.strip()[:200]
                    if user_department is None:  # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –¥–ª—è –∞–¥–º–∏–Ω–∞
                        if chunk_hash in seen_chunks:
                            continue
                        seen_chunks.add(chunk_hash)
                        # –ú–µ—Ç–∫–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –∞–¥–º–∏–Ω–∞
                        if metadata and "department" in metadata:
                            dept_name = metadata["department"]
                            departments_used.add(dept_name)
                            tagged_chunk = f"[–ò—Å—Ç–æ—á–Ω–∏–∫: {dept_name}]\n{chunk}"
                            chunks_texts.append(tagged_chunk)
                        else:
                            chunks_texts.append(chunk)
                    else:
                        chunks_texts.append(chunk)
                
                context = "\n\n---\n\n".join(chunks_texts)
                logger.info(f"[VOICE_RAG] RAG context prepared: {len(context)} chars, {len(chunks_texts)} chunks")
                if user_department is None:
                    logger.info(f"[VOICE_RAG] üè¢ Departments used: {sorted(departments_used)}")
            else:
                # Fallback –µ—Å–ª–∏ –Ω–µ—Ç user_id/session
                logger.warning("[VOICE_RAG] No user_id/session, using fallback context")
                context = GeminiService._load_knowledge_base()
            
            # –≠–¢–ê–ü 3: –ì–ï–ù–ï–†–ê–¶–ò–Ø –û–¢–í–ï–¢–ê –° RAG –ö–û–ù–¢–ï–ö–°–¢–û–ú
            logger.info(f"[VOICE_RAG] Step 3: Generating answer with RAG context ({len(context)} chars)...")
            
            # –°–∏—Å—Ç–µ–º–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è –≥–æ–ª–æ—Å–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
            is_admin = user_department is None if user_id and session else False
            
            system_instruction = """–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ UQsoft. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ø–æ–º–æ–≥–∞—Ç—å —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞–º –Ω–∞—Ö–æ–¥–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –∫–æ–º–ø–∞–Ω–∏–∏.

–ö–†–ò–¢–ò–ß–ù–û - –Ø–ó–´–ö –û–¢–í–ï–¢–ê:
–û–ø—Ä–µ–¥–µ–ª—è–π —è–∑—ã–∫ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –æ—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –Ω–∞ —Ç–æ–º –∂–µ —è–∑—ã–∫–µ (—Ä—É—Å—Å–∫–∏–π, –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –∏–ª–∏ –∫–∏—Ç–∞–π—Å–∫–∏–π). 
–ò—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –Ω–æ –ø–µ—Ä–µ–≤–æ–¥–∏ –µ—ë –Ω–∞ —è–∑—ã–∫ –∑–∞–ø—Ä–æ—Å–∞, –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ.

–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–æ—Å–∏–ª (–≥–æ–ª–æ—Å–æ–º): {question}
–ù–∞–π–¥–∏ –æ—Ç–≤–µ—Ç –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π."""
            
            # –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ô –§–û–†–ú–ê–¢ –î–õ–Ø –ê–î–ú–ò–ù–ê
            if is_admin:
                system_instruction += """

üìÇ –§–û–†–ú–ê–¢ –î–õ–Ø –ê–î–ú–ò–ù–ò–°–¢–†–ê–¢–û–†–ê (–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û):
–¢—ã –≤–∏–¥–∏—à—å —Ç–µ–≥–∏ [–ò—Å—Ç–æ—á–Ω–∏–∫: ...] –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.
–í –ö–û–ù–¶–ï —Å–≤–æ–µ–≥–æ –æ—Ç–≤–µ—Ç–∞ –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –Ω–∞–ø–∏—à–∏:

üìÇ **–ò—Å—Ç–æ—á–Ω–∏–∫–∏:** [—Å–ø–∏—Å–æ–∫ –æ—Ç–¥–µ–ª–æ–≤ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é]"""
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
            prompt_parts = []
            if context:
                prompt_parts.append(f"=== –ö–û–ù–¢–ï–ö–°–¢ –ò–ó –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô ===\n{context}\n\n")
            prompt_parts.append(f"=== –í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø ===\n{transcribed_text}")
            
            prompt = "\n".join(prompt_parts)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = gemini_client.models.generate_content(
                model="gemini-2.5-flash",
                contents=prompt,
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction.replace("{question}", transcribed_text),
                    temperature=0.7,
                )
            )
            
            response_text = response.text if response.text else "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç."
            logger.info(f"[VOICE_RAG] Successfully generated response (length: {len(response_text)})")
            
            return response_text
            
        except Exception as e:
            logger.error(f"[VOICE_RAG] Error: {e}", exc_info=True)
            raise Exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è: {str(e)}")
    
    @staticmethod
    async def _transcribe_audio(
        audio_bytes: bytes | None = None,
        audio_file_path: str | None = None,
        audio_mime_type: str = "audio/ogg"
    ) -> str:
        """
        –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ –≤ —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ Gemini.
        
        Returns:
            –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        try:
            if gemini_client is None:
                raise ValueError("Gemini client not initialized")
            
            content_parts: List = []
            uploaded_file = None
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ —Ñ–∞–π–ª
            if audio_file_path:
                try:
                    uploaded_file = gemini_client.files.upload(file=audio_file_path)
                    content_parts.append(uploaded_file)
                except Exception as upload_error:
                    logger.warning(f"[STT] Failed to upload: {upload_error}, using bytes")
                    with open(audio_file_path, "rb") as f:
                        audio_bytes = f.read()
            
            if not uploaded_file:
                if not audio_bytes:
                    raise ValueError("No audio data provided")
                content_parts.append({
                    "mime_type": audio_mime_type,
                    "data": audio_bytes
                })
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
            content_parts.append("–†–∞—Å–ø–æ–∑–Ω–∞–π —Ä–µ—á—å –∏–∑ —ç—Ç–æ–≥–æ –∞—É–¥–∏–æ –∏ –≤–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û —Ç–µ–∫—Å—Ç —Ç–æ–≥–æ, —á—Ç–æ —Å–∫–∞–∑–∞–ª –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å. –ù–∏—á–µ–≥–æ –∫—Ä–æ–º–µ —Ç–µ–∫—Å—Ç–∞ —Ä–µ—á–∏ –Ω–µ –ø–∏—à–∏.")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
            response = gemini_client.models.generate_content(
                model="gemini-2.5-flash",
                contents=content_parts,
                config=types.GenerateContentConfig(
                    temperature=0.3,
                )
            )
            
            # –û—á–∏—Å—Ç–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            if uploaded_file:
                try:
                    gemini_client.files.delete(name=uploaded_file.name)
                except Exception:
                    pass
            
            transcribed_text = response.text if response.text else ""
            return transcribed_text.strip()
            
        except Exception as e:
            logger.error(f"[STT] Error transcribing audio: {e}", exc_info=True)
            raise
    
    @staticmethod
    def get_answer_from_audio(
        audio_bytes: bytes | None = None,
        audio_file_path: str | None = None,
        audio_mime_type: str = "audio/ogg",
        context: str | None = None
    ) -> str:
        """
        DEPRECATED: –°—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ –±–µ–∑ RAG. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ get_answer_from_audio_with_rag.
        
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞ (–≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è) —á–µ—Ä–µ–∑ –Ω–æ–≤—ã–π API google.genai.
        
        Args:
            audio_bytes: –ë–∞–π—Ç—ã –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω audio_file_path)
            audio_file_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –Ω–∞–¥ audio_bytes)
            audio_mime_type: MIME —Ç–∏–ø –∞—É–¥–∏–æ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é audio/ogg –¥–ª—è Telegram)
            context: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω)
        
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏ Gemini
        
        Raises:
            Exception: –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç
        """
        try:
            logger.info(f"[GEMINI] Starting audio processing: file_path={audio_file_path}, mime_type={audio_mime_type}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª–∏–µ–Ω—Ç–∞
            if gemini_client is None:
                raise ValueError("Gemini client not initialized")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π, –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω
            if context is None:
                logger.info("[GEMINI] Loading knowledge base context...")
                context = GeminiService._load_knowledge_base()
                logger.info(f"[GEMINI] Knowledge base loaded: {len(context)} chars")
            
            # –°–∏—Å—Ç–µ–º–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è —Å –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ—Å—Ç—å—é
            system_instruction = """–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ UQsoft. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ø–æ–º–æ–≥–∞—Ç—å —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞–º –Ω–∞—Ö–æ–¥–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –∫–æ–º–ø–∞–Ω–∏–∏.

–ö–†–ò–¢–ò–ß–ù–û - –Ø–ó–´–ö –û–¢–í–ï–¢–ê:
–û–ø—Ä–µ–¥–µ–ª—è–π —è–∑—ã–∫ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –æ—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –Ω–∞ —Ç–æ–º –∂–µ —è–∑—ã–∫–µ (—Ä—É—Å—Å–∫–∏–π, –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –∏–ª–∏ –∫–∏—Ç–∞–π—Å–∫–∏–π). 
–ò—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –Ω–æ –ø–µ—Ä–µ–≤–æ–¥–∏ –µ—ë –Ω–∞ —è–∑—ã–∫ –∑–∞–ø—Ä–æ—Å–∞, –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ.

–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–∏—Å–ª–∞–ª –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ. –†–∞—Å–ø–æ–∑–Ω–∞–π —Ä–µ—á—å, –Ω–∞–π–¥–∏ –æ—Ç–≤–µ—Ç –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –∏ –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å."""
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏
            content_parts: List = []
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ —Ñ–∞–π–ª —á–µ—Ä–µ–∑ –Ω–æ–≤—ã–π API
            uploaded_file = None
            if audio_file_path:
                try:
                    logger.info(f"[GEMINI] Uploading file via gemini_client.files.upload: {audio_file_path}")
                    uploaded_file = gemini_client.files.upload(file=audio_file_path)
                    logger.info(f"[GEMINI] File uploaded successfully: {uploaded_file.name}")
                    content_parts.append(uploaded_file)
                except Exception as upload_error:
                    logger.warning(f"[GEMINI] Failed to upload file: {upload_error}")
                    logger.info("[GEMINI] Falling back to direct bytes method...")
                    # Fallback: —á–∏—Ç–∞–µ–º —Ñ–∞–π–ª –≤ –±–∞–π—Ç—ã
                    with open(audio_file_path, "rb") as f:
                        audio_bytes = f.read()
                    logger.info(f"[GEMINI] Read file into bytes: {len(audio_bytes)} bytes")
            
            # –°–ø–æ—Å–æ–± 2: –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —á–µ—Ä–µ–∑ upload_file, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–π—Ç—ã
            if not uploaded_file:
                if not audio_bytes:
                    raise ValueError("–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å –ª–∏–±–æ audio_file_path, –ª–∏–±–æ audio_bytes")
                
                logger.info(f"[GEMINI] Using direct bytes method (size: {len(audio_bytes)} bytes)")
                
                # –ü—Ä–æ–±—É–µ–º –æ—Ç–ø—Ä–∞–≤–∏—Ç—å .ogg –Ω–∞–ø—Ä—è–º—É—é
                try:
                    content_parts.append({
                        "mime_type": audio_mime_type,
                        "data": audio_bytes
                    })
                    logger.info(f"[GEMINI] Added audio with mime_type={audio_mime_type}")
                except Exception as ogg_error:
                    logger.warning(f"[GEMINI] Failed to send .ogg directly: {ogg_error}")
                    
                    # –°–ø–æ—Å–æ–± 3: –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º .ogg –≤ .wav —á–µ—Ä–µ–∑ pydub
                    if PYDUB_AVAILABLE and audio_mime_type == "audio/ogg":
                        logger.info("[GEMINI] Attempting to convert .ogg to .wav using pydub...")
                        try:
                            import tempfile
                            temp_wav = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
                            temp_wav_path = temp_wav.name
                            temp_wav.close()
                            
                            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º
                            if audio_file_path:
                                audio_segment = AudioSegment.from_file(audio_file_path, format="ogg")
                            else:
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞–π—Ç—ã –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
                                temp_ogg = tempfile.NamedTemporaryFile(suffix=".ogg", delete=False)
                                temp_ogg.write(audio_bytes)
                                temp_ogg_path = temp_ogg.name
                                temp_ogg.close()
                                
                                audio_segment = AudioSegment.from_file(temp_ogg_path, format="ogg")
                                Path(temp_ogg_path).unlink()  # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π .ogg
                            
                            audio_segment.export(temp_wav_path, format="wav")
                            logger.info(f"[GEMINI] Converted to WAV: {temp_wav_path}")
                            
                            # –ß–∏—Ç–∞–µ–º .wav –≤ –±–∞–π—Ç—ã
                            with open(temp_wav_path, "rb") as f:
                                wav_bytes = f.read()
                            
                            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                            Path(temp_wav_path).unlink()
                            
                            # –ò—Å–ø–æ–ª—å–∑—É–µ–º .wav
                            content_parts.append({
                                "mime_type": "audio/wav",
                                "data": wav_bytes
                            })
                            logger.info(f"[GEMINI] Using converted WAV (size: {len(wav_bytes)} bytes)")
                        except Exception as convert_error:
                            logger.error(f"[GEMINI] Failed to convert to WAV: {convert_error}", exc_info=True)
                            raise Exception(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∞—É–¥–∏–æ —Ñ–∞–π–ª: {str(convert_error)}")
                    else:
                        if not PYDUB_AVAILABLE:
                            logger.warning("[GEMINI] pydub not available, cannot convert audio")
                        raise Exception(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –∞—É–¥–∏–æ –≤ Gemini: {str(ogg_error)}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
            prompt_text = "–†–∞—Å–ø–æ–∑–Ω–∞–π —Ä–µ—á—å –≤ –∞—É–¥–∏–æ –∏ –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –∏—Å–ø–æ–ª—å–∑—É—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π."
            if context:
                prompt_text += f"\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:\n{context}"
            
            content_parts.append(prompt_text)
            logger.info(f"[GEMINI] Added text prompt (length: {len(prompt_text)} chars)")
            
            logger.info(f"[GEMINI] Generating response with {len(content_parts)} content parts...")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ –Ω–æ–≤—ã–π API
            response = gemini_client.models.generate_content(
                model="gemini-2.5-flash",
                contents=content_parts,
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction,
                    temperature=0.7,
                )
            )
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
            response_text = response.text if response.text else "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å –∏–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç."
            
            logger.info(f"[GEMINI] Successfully generated response (length: {len(response_text)} chars)")
            
            # –£–¥–∞–ª—è–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ upload_file
            if uploaded_file:
                try:
                    gemini_client.files.delete(name=uploaded_file.name)
                    logger.info(f"[GEMINI] Cleaned up uploaded file: {uploaded_file.name}")
                except Exception as cleanup_error:
                    logger.warning(f"[GEMINI] Failed to cleanup uploaded file: {cleanup_error}")
            
            return response_text
        
        except ResourceExhausted as e:
            logger.warning(f"[GEMINI] Quota exceeded (429) for audio: {e}")
            return QUOTA_EXCEEDED_MESSAGE
            
        except Exception as e:
            logger.error(f"[GEMINI] ERROR generating response from audio: {type(e).__name__}: {e}", exc_info=True)
            raise Exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∞—É–¥–∏–æ: {str(e)}")
    
    @staticmethod
    def extract_media_links(text: str) -> dict[str, List[str]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ–¥–∏–∞-—Å—Å—ã–ª–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ (YouTube, —Ñ–∞–π–ª—ã –∏ —Ç.–¥.).
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ç–∏–ø–∞–º–∏ —Å—Å—ã–ª–æ–∫ –∏ –∏—Ö –∑–Ω–∞—á–µ–Ω–∏—è–º–∏:
            {
                "youtube": ["https://youtube.com/watch?v=..."],
                "files": ["https://example.com/file.pdf"],
                "images": ["https://example.com/image.png"]
            }
        """
        import re
        
        media_links: dict[str, List[str]] = {
            "youtube": [],
            "files": [],
            "images": []
        }
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫
        youtube_pattern = r'(?:https?://)?(?:www\.)?(?:youtube\.com/watch\?v=|youtu\.be/)([a-zA-Z0-9_-]{11})'
        file_pattern = r'https?://[^\s]+\.(?:pdf|doc|docx|xls|xlsx|ppt|pptx|zip|rar|txt|md)'
        image_pattern = r'https?://[^\s]+\.(?:jpg|jpeg|png|gif|webp|bmp|svg)'
        
        # –ò—â–µ–º YouTube —Å—Å—ã–ª–∫–∏
        youtube_matches = re.findall(youtube_pattern, text, re.IGNORECASE)
        for match in youtube_matches:
            full_url = f"https://www.youtube.com/watch?v={match}"
            if full_url not in media_links["youtube"]:
                media_links["youtube"].append(full_url)
        
        # –ò—â–µ–º —Ñ–∞–π–ª—ã
        file_matches = re.findall(file_pattern, text, re.IGNORECASE)
        media_links["files"].extend(file_matches)
        
        # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image_matches = re.findall(image_pattern, text, re.IGNORECASE)
        media_links["images"].extend(image_matches)
        
        return media_links
    
    @staticmethod
    def process_knowledge_text(raw_text: str) -> tuple[str, str]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π.
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç.
        
        Args:
            raw_text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
        Returns:
            –ö–æ—Ä—Ç–µ–∂ (filename, structured_text):
            - filename: –ö–æ—Ä–æ—Ç–∫–æ–µ –∞–Ω–≥–ª–∏–π—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ (–±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è)
            - structured_text: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        
        Raises:
            Exception: –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–µ–∫—Å—Ç
        """
        try:
            logger.info(f"[GEMINI] Processing knowledge text (length: {len(raw_text)} chars)...")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ API –∫–ª—é—á–∞
            if not settings.gemini_api_key:
                raise ValueError("GEMINI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –µ–≥–æ –≤ .env —Ñ–∞–π–ª–µ.")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª–∏–µ–Ω—Ç–∞
            if gemini_client is None:
                raise ValueError("Gemini client not initialized")
            
            # –°–∏—Å—Ç–µ–º–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞–∑–≤–∞–Ω–∏—è –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è
            system_instruction = """–¢—ã ‚Äî AI-—Ä–µ–¥–∞–∫—Ç–æ—Ä –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∫–æ–º–ø–∞–Ω–∏–∏ UQsoft.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞:
1. –ü—Ä–∏–¥—É–º–∞—Ç—å –∫–æ—Ä–æ—Ç–∫–æ–µ –∞–Ω–≥–ª–∏–π—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ (snake_case, –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è, –º–∞–∫—Å–∏–º—É–º 3 —Å–ª–æ–≤–∞)
2. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç: –¥–æ–±–∞–≤–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∏, —Å–ø–∏—Å–∫–∏, –≤—ã–¥–µ–ª–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã
3. –í–ê–ñ–ù–û: –°–æ—Ö—Ä–∞–Ω–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞ (–Ω–µ –ø–µ—Ä–µ–≤–æ–¥–∏ –∫–æ–Ω—Ç–µ–Ω—Ç)

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (—Å—Ç—Ä–æ–≥–æ —Å–æ–±–ª—é–¥–∞–π):
FILENAME: –Ω–∞–∑–≤–∞–Ω–∏–µ_—Ñ–∞–π–ª–∞
---
–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∑–¥–µ—Å—å..."""
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
            prompt = f"–û–±—Ä–∞–±–æ—Ç–∞–π —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:\n\n{raw_text}"
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ –Ω–æ–≤—ã–π API
            logger.info("[GEMINI] Generating structured text with gemini-2.5-flash...")
            response = gemini_client.models.generate_content(
                model="gemini-2.5-flash",
                contents=prompt,
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction,
                    temperature=0.5,
                )
            )
            
            if not response.text:
                raise ValueError("Gemini –Ω–µ –≤–µ—Ä–Ω—É–ª –æ—Ç–≤–µ—Ç")
            
            response_text = response.text.strip()
            logger.info(f"[GEMINI] Received response (length: {len(response_text)} chars)")
            
            # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç
            if "FILENAME:" not in response_text or "---" not in response_text:
                # –ï—Å–ª–∏ —Ñ–æ—Ä–º–∞—Ç –Ω–µ —Å–æ–±–ª—é–¥–µ–Ω, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–∑ –ø–µ—Ä–≤—ã—Ö —Å–ª–æ–≤
                import re
                words = re.findall(r'\b[a-zA-Z–∞-—è–ê-–Ø]+\b', raw_text[:100])[:3]
                filename = "_".join(words).lower()[:30] if words else "knowledge_doc"
                structured_text = response_text
            else:
                # –ü–∞—Ä—Å–∏–º FILENAME –∏ —Ç–µ–∫—Å—Ç
                parts = response_text.split("---", 1)
                filename_line = parts[0].strip()
                structured_text = parts[1].strip() if len(parts) > 1 else raw_text
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º filename
                if "FILENAME:" in filename_line:
                    filename = filename_line.replace("FILENAME:", "").strip()
                else:
                    filename = "knowledge_doc"
            
            # –û—á–∏—â–∞–µ–º filename –æ—Ç –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
            import re
            filename = re.sub(r'[^\w\-]', '_', filename).lower()
            filename = re.sub(r'_+', '_', filename).strip('_')[:50]
            
            if not filename:
                filename = "knowledge_doc"
            
            logger.info(f"[GEMINI] Generated filename: {filename}")
            
            return filename, structured_text
        
        except ResourceExhausted as e:
            logger.warning(f"[GEMINI] Quota exceeded (429) for knowledge processing: {e}")
            raise Exception(QUOTA_EXCEEDED_MESSAGE)
            
        except Exception as e:
            logger.error(f"[GEMINI] Error processing knowledge text: {e}", exc_info=True)
            raise Exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞: {str(e)}")
    
    @staticmethod
    async def process_knowledge_audio(audio_path: Path) -> tuple[str, str]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Gemini 2.5 Flash Native Audio –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è.
        
        Args:
            audio_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ-—Ñ–∞–π–ª—É (.ogg)
        
        Returns:
            –ö–æ—Ä—Ç–µ–∂ (filename, structured_text):
            - filename: –ö–æ—Ä–æ—Ç–∫–æ–µ –∞–Ω–≥–ª–∏–π—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ (–±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è)
            - structured_text: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–∑ –∞—É–¥–∏–æ
        
        Raises:
            Exception: –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∞—É–¥–∏–æ
        """
        try:
            logger.info(f"[GEMINI] Processing knowledge audio: {audio_path.name}...")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª–∏–µ–Ω—Ç–∞
            if gemini_client is None:
                raise ValueError("Gemini client not initialized")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ-—Ñ–∞–π–ª
            logger.info(f"[GEMINI] Uploading audio file to Gemini...")
            uploaded_file = gemini_client.files.upload(file=str(audio_path))
            logger.info(f"[GEMINI] Audio file uploaded: {uploaded_file.name}")
            
            # –°–∏—Å—Ç–µ–º–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ
            system_instruction = """–¢—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ UQsoft. –ü—Ä–µ–≤—Ä–∞—Ç–∏ —ç—Ç–æ –∞—É–¥–∏–æ –≤ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π.
–£–¥–∞–ª–∏ –º—É—Å–æ—Ä, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –ø–æ –ø—É–Ω–∫—Ç–∞–º:
‚Ä¢ –°—É—Ç—å
‚Ä¢ –î–µ—Ç–∞–ª–∏
‚Ä¢ –¢–µ–≥–∏
‚Ä¢ –ó–∞–¥–∞—á–∏

–í–ê–ñ–ù–û: –°–æ—Ö—Ä–∞–Ω–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —è–∑—ã–∫ –∞—É–¥–∏–æ (–Ω–µ –ø–µ—Ä–µ–≤–æ–¥–∏ –∫–æ–Ω—Ç–µ–Ω—Ç).

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (—Å—Ç—Ä–æ–≥–æ —Å–æ–±–ª—é–¥–∞–π):
FILENAME: –∫–æ—Ä–æ—Ç–∫–æ–µ_–Ω–∞–∑–≤–∞–Ω–∏–µ
---
# –°—É—Ç—å
[–æ—Å–Ω–æ–≤–Ω–∞—è –º—ã—Å–ª—å]

# –î–µ—Ç–∞–ª–∏
[–ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏]

# –¢–µ–≥–∏
[–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞]

# –ó–∞–¥–∞—á–∏
[–¥–µ–π—Å—Ç–≤–∏—è, –µ—Å–ª–∏ –µ—Å—Ç—å]
"""
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ –Ω–æ–≤—ã–π API —Å multimodal support (gemini-2.5-flash –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∞—É–¥–∏–æ)
            logger.info("[GEMINI] Generating structured text from audio with gemini-2.5-flash...")
            response = gemini_client.models.generate_content(
                model="gemini-2.5-flash",
                contents=[
                    "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ –∞—É–¥–∏–æ –∏ —Å–æ–∑–¥–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞–Ω–∏–µ –¥–ª—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.",
                    uploaded_file,
                ],
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction,
                    temperature=0.5,
                )
            )
            
            if not response.text:
                raise ValueError("Gemini –Ω–µ –≤–µ—Ä–Ω—É–ª –æ—Ç–≤–µ—Ç –∏–∑ –∞—É–¥–∏–æ")
            
            response_text = response.text.strip()
            logger.info(f"[GEMINI] Received structured text from audio (length: {len(response_text)} chars)")
            
            # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç
            if "FILENAME:" not in response_text or "---" not in response_text:
                # –ï—Å–ª–∏ —Ñ–æ—Ä–º–∞—Ç –Ω–µ —Å–æ–±–ª—é–¥–µ–Ω, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
                import re
                from datetime import datetime
                timestamp = datetime.now().strftime("%Y%m%d_%H%M")
                filename = f"audio_knowledge_{timestamp}"
                structured_text = response_text
            else:
                # –ü–∞—Ä—Å–∏–º FILENAME –∏ —Ç–µ–∫—Å—Ç
                parts = response_text.split("---", 1)
                filename_line = parts[0].strip()
                structured_text = parts[1].strip() if len(parts) > 1 else response_text
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º filename
                if "FILENAME:" in filename_line:
                    filename = filename_line.replace("FILENAME:", "").strip()
                else:
                    from datetime import datetime
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
                    filename = f"audio_knowledge_{timestamp}"
            
            # –û—á–∏—â–∞–µ–º filename –æ—Ç –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
            import re
            filename = re.sub(r'[^\w\-]', '_', filename).lower()
            filename = re.sub(r'_+', '_', filename).strip('_')[:50]
            
            if not filename:
                from datetime import datetime
                timestamp = datetime.now().strftime("%Y%m%d_%H%M")
                filename = f"audio_knowledge_{timestamp}"
            
            logger.info(f"[GEMINI] Generated filename from audio: {filename}")
            
            # –£–¥–∞–ª—è–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –∏–∑ Gemini
            try:
                gemini_client.files.delete(name=uploaded_file.name)
                logger.info(f"[GEMINI] Deleted uploaded audio file: {uploaded_file.name}")
            except Exception as e:
                logger.warning(f"[GEMINI] Failed to delete uploaded file: {e}")
            
            return filename, structured_text
        
        except ResourceExhausted as e:
            logger.warning(f"[GEMINI] Quota exceeded (429) for audio processing: {e}")
            raise Exception(QUOTA_EXCEEDED_MESSAGE)
            
        except Exception as e:
            logger.error(f"[GEMINI] Error processing knowledge audio: {e}", exc_info=True)
            raise Exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∞—É–¥–∏–æ: {str(e)}")
    
    @staticmethod
    def get_knowledge_files() -> List[str]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ –ø–∞–ø–∫–∏ knowledge.
        
        Returns:
            –°–ø–∏—Å–æ–∫ –∏–º–µ–Ω —Ñ–∞–π–ª–æ–≤ (–±–µ–∑ –ø—É—Ç–µ–π)
        """
        try:
            knowledge_path = Path("data/knowledge")
            if not knowledge_path.exists():
                logger.info("Knowledge base directory does not exist")
                return []
            
            files = [
                file.name
                for file in knowledge_path.iterdir()
                if file.is_file() and file.suffix in {".txt", ".pdf", ".md", ".rst"}
            ]
            
            logger.info(f"Found {len(files)} knowledge files")
            return sorted(files)
            
        except Exception as e:
            logger.error(f"Error getting knowledge files: {e}", exc_info=True)
            return []
    
    @staticmethod
    def delete_knowledge_file(filename: str) -> bool:
        """
        –£–¥–∞–ª—è–µ—Ç —Ñ–∞–π–ª –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.
        
        Args:
            filename: –ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è (–±–µ–∑ –ø—É—Ç–∏)
        
        Returns:
            True –µ—Å–ª–∏ —Ñ–∞–π–ª —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω, False –µ—Å–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞
        
        Raises:
            Exception: –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏
        """
        try:
            knowledge_path = Path("data/knowledge")
            if not knowledge_path.exists():
                raise FileNotFoundError("Knowledge base directory does not exist")
            
            file_path = knowledge_path / filename
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            if not file_path.exists():
                raise FileNotFoundError(f"File {filename} does not exist in knowledge base")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –≤ knowledge –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ (–∑–∞—â–∏—Ç–∞ –æ—Ç path traversal)
            if not str(file_path.resolve()).startswith(str(knowledge_path.resolve())):
                raise ValueError(f"Invalid file path: {filename}")
            
            # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª
            file_path.unlink()
            
            logger.info(f"Deleted knowledge file: {filename}")
            return True
            
        except Exception as e:
            logger.error(f"Error deleting knowledge file {filename}: {e}", exc_info=True)
            raise Exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {str(e)}")
    
    @staticmethod
    def get_knowledge_stats() -> dict[str, int]:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–∞–∂–¥–æ–º –æ—Ç–¥–µ–ª–µ.
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å {department_name: file_count}, –Ω–∞–ø—Ä–∏–º–µ—Ä:
            {
                "common": 5,
                "sorting": 12,
                "delivery": 8,
                "manager": 3,
                "customer_service": 7
            }
        """
        try:
            knowledge_path = Path("data/knowledge")
            if not knowledge_path.exists():
                logger.info("[STATS] Knowledge base directory does not exist")
                return {}
            
            # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã —Ñ–∞–π–ª–æ–≤
            supported_extensions = {".txt", ".pdf", ".docx", ".md", ".rst"}
            
            stats: dict[str, int] = {}
            
            # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º –ø–∞–ø–∫–∞–º (–æ—Ç–¥–µ–ª–∞–º) –≤ data/knowledge
            for dept_path in knowledge_path.iterdir():
                if not dept_path.is_dir():
                    continue
                
                dept_name = dept_path.name
                
                # –°—á–∏—Ç–∞–µ–º —Ñ–∞–π–ª—ã —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ (–≤–∫–ª—é—á–∞—è –ø–æ–¥–ø–∞–ø–∫–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä delivery/courier)
                file_count = 0
                for file_path in dept_path.rglob("*"):
                    if file_path.is_file() and file_path.suffix.lower() in supported_extensions:
                        file_count += 1
                
                if file_count > 0:
                    stats[dept_name] = file_count
                    logger.info(f"[STATS] Department '{dept_name}': {file_count} files")
            
            logger.info(f"[STATS] Total departments with files: {len(stats)}")
            return stats
            
        except Exception as e:
            logger.error(f"[STATS] Error getting knowledge stats: {e}", exc_info=True)
            return {}
    
    @staticmethod
    def get_department_files(dept_name: str) -> List[dict[str, str]]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º –æ—Ç–¥–µ–ª–µ.
        
        Args:
            dept_name: –ù–∞–∑–≤–∞–Ω–∏–µ –æ—Ç–¥–µ–ª–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'sorting', 'manager')
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ñ–∞–π–ª–∞—Ö:
            [
                {
                    "name": "guide.txt",
                    "path": "sorting/guide.txt",
                    "size": "1.2 KB",
                    "size_bytes": 1234
                },
                ...
            ]
        """
        try:
            knowledge_path = Path("data/knowledge")
            dept_path = knowledge_path / dept_name
            
            if not dept_path.exists() or not dept_path.is_dir():
                logger.warning(f"[FILES] Department '{dept_name}' not found")
                return []
            
            # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã —Ñ–∞–π–ª–æ–≤
            supported_extensions = {".txt", ".pdf", ".docx", ".md", ".rst"}
            
            files_info: List[dict[str, str]] = []
            
            # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∏—â–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –≤ –æ—Ç–¥–µ–ª–µ
            for file_path in dept_path.rglob("*"):
                if file_path.is_file() and file_path.suffix.lower() in supported_extensions:
                    # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –æ—Ç knowledge/
                    relative_path = file_path.relative_to(knowledge_path)
                    
                    # –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –≤ –±–∞–π—Ç–∞—Ö
                    size_bytes = file_path.stat().st_size
                    
                    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
                    if size_bytes < 1024:
                        size_str = f"{size_bytes} B"
                    elif size_bytes < 1024 * 1024:
                        size_str = f"{size_bytes / 1024:.1f} KB"
                    else:
                        size_str = f"{size_bytes / (1024 * 1024):.1f} MB"
                    
                    files_info.append({
                        "name": file_path.name,
                        "path": str(relative_path).replace("\\", "/"),
                        "size": size_str,
                        "size_bytes": size_bytes
                    })
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            files_info.sort(key=lambda x: x["name"].lower())
            
            logger.info(f"[FILES] Found {len(files_info)} files in department '{dept_name}'")
            return files_info
            
        except Exception as e:
            logger.error(f"[FILES] Error getting files for department '{dept_name}': {e}", exc_info=True)
            return []
    
    @staticmethod
    def delete_document(dept_name: str, filename: str) -> bool:
        """
        –£–¥–∞–ª—è–µ—Ç —Ñ–∞–π–ª –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã.
        
        Args:
            dept_name: –ù–∞–∑–≤–∞–Ω–∏–µ –æ—Ç–¥–µ–ª–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'sorting', 'manager')
            filename: –ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
        
        Returns:
            True –µ—Å–ª–∏ —Ñ–∞–π–ª —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω, False –ø—Ä–∏ –æ—à–∏–±–∫–µ
        
        Raises:
            FileNotFoundError: –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            ValueError: –ï—Å–ª–∏ –ø—É—Ç—å –Ω–µ–≤–∞–ª–∏–¥–µ–Ω (–∑–∞—â–∏—Ç–∞ –æ—Ç path traversal)
            Exception: –ü—Ä–∏ –¥—Ä—É–≥–∏—Ö –æ—à–∏–±–∫–∞—Ö
        """
        try:
            knowledge_path = Path("data/knowledge")
            if not knowledge_path.exists():
                raise FileNotFoundError("Knowledge base directory does not exist")
            
            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å: –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ dept_name –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
            if ".." in dept_name or "/" in dept_name or "\\" in dept_name:
                raise ValueError(f"Invalid department name: {dept_name}")
            
            dept_path = knowledge_path / dept_name
            if not dept_path.exists() or not dept_path.is_dir():
                raise FileNotFoundError(f"Department '{dept_name}' not found")
            
            # –ò—â–µ–º —Ñ–∞–π–ª (–º–æ–∂–µ—Ç –±—ã—Ç—å –≤ –ø–æ–¥–ø–∞–ø–∫–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä delivery/courier/)
            file_path = None
            for found_file in dept_path.rglob(filename):
                if found_file.is_file() and found_file.name == filename:
                    file_path = found_file
                    break
            
            if not file_path:
                raise FileNotFoundError(f"File '{filename}' not found in department '{dept_name}'")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –≤ knowledge –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ (–∑–∞—â–∏—Ç–∞ –æ—Ç path traversal)
            if not str(file_path.resolve()).startswith(str(knowledge_path.resolve())):
                raise ValueError(f"Invalid file path: {filename}")
            
            logger.info(f"[DELETE] Deleting file: {file_path}")
            
            # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª
            file_path.unlink()
            logger.info(f"[DELETE] File deleted successfully: {file_path}")
            
            # –ü–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ–º –∏–Ω–¥–µ–∫—Å –æ—Ç–¥–µ–ª–∞
            try:
                logger.info(f"[DELETE] Rebuilding index for department: {dept_name}")
                GeminiService.rebuild_index_for_department(dept_name)
                logger.info(f"[DELETE] Index rebuilt successfully for department: {dept_name}")
            except Exception as rebuild_error:
                logger.error(f"[DELETE] Error rebuilding index: {rebuild_error}", exc_info=True)
                # –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å, —Ñ–∞–π–ª —É–∂–µ —É–¥–∞–ª–µ–Ω
            
            return True
            
        except Exception as e:
            logger.error(f"[DELETE] Error deleting document '{filename}' from '{dept_name}': {e}", exc_info=True)
            raise


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ –ø–æ –æ—Ç–¥–µ–ª–∞–º –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥—É–ª—è
try:
    GeminiService._create_department_indices()
except Exception as e:
    logger.error(f"[RAG] Failed to initialize department indices: {e}", exc_info=True)


# –û—Å—Ç–∞–≤–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ (—É—Å—Ç–∞—Ä–µ–ª–∞, —Ç—Ä–µ–±—É–µ—Ç user_id –∏ session)
def generate_response(prompt: str, context: str | None = None) -> str:
    """
    –£—Å—Ç–∞—Ä–µ–≤—à–∞—è —Ñ—É–Ω–∫—Ü–∏—è. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ GeminiService.get_answer() –≤–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ.
    –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –±–æ–ª—å—à–µ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, —Ç–∞–∫ –∫–∞–∫ get_answer —Ç–µ–ø–µ—Ä—å —Ç—Ä–µ–±—É–µ—Ç user_id –∏ session.
    """
    raise NotImplementedError(
        "generate_response() —É—Å—Ç–∞—Ä–µ–ª–∞. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ GeminiService.get_answer(prompt, user_id, session, context)"
    )
